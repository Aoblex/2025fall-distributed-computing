FROM ubuntu:22.04

USER root

# Allow build systems (buildx) to inject TARGETARCH automatically.
# Fallback to amd64 when not provided so local `docker build` still works.
ARG TARGETARCH
ENV HADOOP_VERSION=3.3.6 \
    HADOOP_HOME=/opt/hadoop \
    JAVA_HOME=/usr/lib/jvm/java-8-openjdk-${TARGETARCH}/ \
    SPARK_VERSION=3.5.6 \
    SPARK_HOME=/opt/spark
ENV HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
ENV HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin

RUN sed -i 's/deb.debian.org/mirrors.ustc.edu.cn/g' /etc/apt/sources.list
RUN sed -i 's/security.debian.org/mirrors.ustc.edu.cn/g' /etc/apt/sources.list
RUN sed -i 's/ports.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list

RUN apt-get update && \
    apt-get install -y wget tar openssh-server openjdk-8-jdk && \
    apt-get clean

RUN wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz && \
    tar -xzf hadoop-${HADOOP_VERSION}.tar.gz -C /opt/ && \
    mv /opt/hadoop-${HADOOP_VERSION} /opt/hadoop && \
    rm hadoop-${HADOOP_VERSION}.tar.gz

# Run Hadoop daemons as root inside container
ENV HDFS_NAMENODE_USER=root \
    HDFS_DATANODE_USER=root \
    YARN_RESOURCEMANAGER_USER=root \
    YARN_NODEMANAGER_USER=root \
    MAPRED_HISTORYSERVER_USER=root

# Copy Hadoop configuration
ARG HADOOP_CONFIG=docker/config/hadoop
COPY ${HADOOP_CONFIG}/core-site.xml ${HADOOP_CONF_DIR}/core-site.xml
COPY ${HADOOP_CONFIG}/hdfs-site.xml ${HADOOP_CONF_DIR}/hdfs-site.xml
COPY ${HADOOP_CONFIG}/yarn-site.xml ${HADOOP_CONF_DIR}/yarn-site.xml
COPY ${HADOOP_CONFIG}/mapred-site.xml ${HADOOP_CONF_DIR}/mapred-site.xml

# Install Spark
RUN wget https://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3-scala2.13.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop3.tgz -C /opt/ && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop3 ${SPARK_HOME} && \
    rm spark-${SPARK_VERSION}-bin-hadoop3.tgz

RUN ssh-keygen -q -t rsa -N '' -f /root/.ssh/id_rsa && \
    cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys && \
    chmod 600 /root/.ssh/authorized_keys

# create sshd run dir
RUN mkdir -p /var/run/sshd

# set working directory
WORKDIR /opt/hadoop
CMD ["bash"]
