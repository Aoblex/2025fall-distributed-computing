# ============================================================
# Base Image
# ============================================================

FROM ubuntu:22.04
USER root


# ============================================================
# Environment Variables
# ============================================================


# Set environment variables
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/ \
    HADOOP_VERSION=3.3.6 \
    HADOOP_HOME=/opt/hadoop \
    HADOOP_COMMON_LIB_NATIVE_DIR=/opt/hadoop/lib/native \
    HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop \
    LD_LIBRARY_PATH=/opt/hadoop/lib/native \
    HADOOP_OPTS="-Djava.library.path=/opt/hadoop/lib/native" \
    SPARK_VERSION=3.5.6 \
    SPARK_HOME=/opt/spark \
    CONDA_DIR=/opt/miniconda \
    PATH=$PATH:/opt/hadoop/bin:/opt/hadoop/sbin:/opt/spark/bin:/opt/spark/sbin:/opt/miniconda/bin

# ============================================================
# System Configuration and Dependencies
# ============================================================

# Use faster Chinese mirrors (USTC)
RUN sed -i 's|archive.ubuntu.com|mirrors.ustc.edu.cn|g' /etc/apt/sources.list && \
    sed -i 's|security.ubuntu.com|mirrors.ustc.edu.cn|g' /etc/apt/sources.list

# Install base tools
RUN apt-get update && \
    apt-get install -y \
        wget curl tar make \
        vim git less \
        openssh-server \
        bsdmainutils \
        openjdk-8-jdk \
        libsnappy1v5 libbz2-1.0 \
        libzstd1 libssl3 && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*


# ============================================================
# Hadoop Installation
# ============================================================

# Download and extract Hadoop
RUN wget -q https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz && \
    tar -xzf hadoop-${HADOOP_VERSION}.tar.gz -C /opt/ && \
    mv /opt/hadoop-${HADOOP_VERSION} /opt/hadoop && \
    rm hadoop-${HADOOP_VERSION}.tar.gz

# Run Hadoop daemons as root inside container
ENV HDFS_NAMENODE_USER=root \
    HDFS_DATANODE_USER=root \
    YARN_RESOURCEMANAGER_USER=root \
    YARN_NODEMANAGER_USER=root \
    MAPRED_HISTORYSERVER_USER=root

# Copy Hadoop configuration
ARG HADOOP_CONFIG=docker/config/hadoop
COPY ${HADOOP_CONFIG}/core-site.xml ${HADOOP_CONF_DIR}/core-site.xml
COPY ${HADOOP_CONFIG}/hdfs-site.xml ${HADOOP_CONF_DIR}/hdfs-site.xml
COPY ${HADOOP_CONFIG}/yarn-site.xml ${HADOOP_CONF_DIR}/yarn-site.xml
COPY ${HADOOP_CONFIG}/mapred-site.xml ${HADOOP_CONF_DIR}/mapred-site.xml


# ============================================================
# Spark Installation
# ============================================================

# Download and extract Spark
RUN wget -q https://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3-scala2.13.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop3-scala2.13.tgz -C /opt/ && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop3-scala2.13 /opt/spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop3-scala2.13.tgz


# ============================================================
# SSH Configuration
# ============================================================

# Configure SSH for Hadoop
RUN ssh-keygen -q -t rsa -N '' -f /root/.ssh/id_rsa && \
    cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys && \
    chmod 600 /root/.ssh/authorized_keys && \
    mkdir -p /var/run/sshd


# ============================================================
# Conda & Python Setup
# ============================================================

# Install Miniconda
RUN wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh && \
    bash /tmp/miniconda.sh -b -p $CONDA_DIR && \
    rm /tmp/miniconda.sh

# Accept Anaconda TOS
RUN $CONDA_DIR/bin/conda tos accept \
        --override-channels --channel \
        https://repo.anaconda.com/pkgs/main && \
    $CONDA_DIR/bin/conda tos accept \
        --override-channels --channel \
        https://repo.anaconda.com/pkgs/r

# Configure Conda
RUN $CONDA_DIR/bin/conda init --all && \
    $CONDA_DIR/bin/conda config --set show_channel_urls yes && \
    $CONDA_DIR/bin/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main && \
    $CONDA_DIR/bin/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free && \
    $CONDA_DIR/bin/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r && \
    $CONDA_DIR/bin/conda install -y pip && \
    $CONDA_DIR/bin/conda clean -a -y

# Install Python packages
RUN pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple && \
    pip install --no-cache-dir pandas numpy pyarrow "pyspark==3.2.4"


# ============================================================
# Final Configurations
# ============================================================

WORKDIR /opt/hadoop
CMD ["bash"]
